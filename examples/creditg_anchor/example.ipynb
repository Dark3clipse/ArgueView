{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# GreditG ArgueView Example\n",
    "\n",
    "This example showcases the use of ArgueView in a realistic usage scenario. We will use the \n",
    "following tools:\n",
    "- [CreditG dataset](https://www.openml.org/d/31)\n",
    "- scikit-learn's RandomForest classifier\n",
    "- [Anchor](https://github.com/marcotcr/anchor)'s black-box explainer\n",
    "\n",
    "![Procedure visualization](https://github.com/sophiahadash/argueview/blob/master/screenshots/model.png?raw=true)\n",
    "\n",
    "Make sure to install all development dependencies prior to running this example: \n",
    "`pipenv install --dev`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from anchor import utils\n",
    "from anchor import anchor_tabular\n",
    "import numpy as np\n",
    "import openml as oml\n",
    "import pandas as pd\n",
    "import requests\n",
    "from openml import OpenMLDataset\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import *\n",
    "from sklearn.model_selection import *\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import *\n",
    "from argueview.typings import Source, OpenMLFeatureData, Case, CaseSource, CaseFeature\n",
    "from argueview.visualizers import ToulminVisualizer, FeatureListVisualizer\n",
    "from examples.Dataset import Dataset\n",
    "from argueview import *\n",
    "from argueview.helper import feature_importance_from_lime, feature_importance_from_anchor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The data\n",
    "The [CreditG dataset](https://www.openml.org/d/31) is a dataset from 1994 of loan applications in \n",
    "Germany. It classifies people described by a set of attributes as good or bad credit risks. The\n",
    "dataset contains 1000 observations, 2 classes (loan eligible / loan ineligible), and 20 features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Loading the dataset\n",
    "Let's start by loading the dataset. First, we need to specify our OpenML API key. You can store\n",
    "the key in your `.env` file or directly set the key in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "oml.config.apikey = os.getenv('OML_APIKEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Then we download the dataset from the OpenML server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# CreditG OpenML identifier\n",
    "dset = 31\n",
    "\n",
    "# Download credit-g data\n",
    "D: OpenMLDataset = oml.datasets.get_dataset(dset)\n",
    "\n",
    "# extract data\n",
    "X, y, C, F = D.get_data(target=D.default_target_attribute, dataset_format='array')\n",
    "y_labels = D.retrieve_class_labels()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We split the data in a training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, test_size=1/7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The OpenML `get_data` method retrieves the CreditG dataset, but does not download descriptive \n",
    "information about the features. We would like to have these descriptives later on in the\n",
    "example, so we download them here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "Unverified HTTPS request is being made to host 'www.openml.org'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "def openml_get_data_features(dataset_id: int) -> any:\n",
    "\tendpoint = \"https://www.openml.org/api/v1/json/data/features/\" + str(\n",
    "\t\tdataset_id) + \"?api_key=\" + oml.config.apikey\n",
    "\theaders = {'content-type': 'application/json', 'Accept-Charset': 'UTF-8'}\n",
    "\tr = requests.get(endpoint, headers=headers, verify=False)\n",
    "\tif r.status_code == 200:\n",
    "\t\tloads = json.loads(r.text)\n",
    "\t\tfor f in loads['data_features']['feature']:\n",
    "\t\t\tf['index'] = int(f['index'])\n",
    "\t\t\tf['is_target'] = bool(f['is_target'])\n",
    "\t\t\tf['is_ignore'] = bool(f['is_ignore'])\n",
    "\t\t\tf['is_row_identifier'] = bool(f['is_row_identifier'])\n",
    "\t\t\tf['number_of_missing_values'] = int(f['number_of_missing_values'])\n",
    "\t\treturn loads\n",
    "\telse:\n",
    "\t\treturn \"\"\n",
    "rt = openml_get_data_features(dset)\n",
    "dfeature = OpenMLFeatureData(rt['data_features'])\n",
    "dfeature.feature.remove(dfeature.feature[20])\n",
    "\n",
    "#print(json.dumps(dfeature.feature, indent = 3))\n",
    "\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Unfortunately the dataset's wiki page contains feature descriptions that are not included\n",
    "in the feature data. Let's add them manually for completeness:\n",
    "\t\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "d = dfeature.feature\n",
    "d[0][\"description\"] = \"Status of existing checking account, in Deutsche Mark.\"\n",
    "d[1][\"description\"] = \"Duration in months\"\n",
    "d[2][\"description\"] = \"Credit history (credits taken, paid back duly, delays, critical accounts)\"\n",
    "d[3][\"description\"] = \"Purpose of the credit (car, television,...)\"\n",
    "d[4][\"description\"] = \"Credit amount\"\n",
    "d[5][\"description\"] = \"Status of savings account/bonds, in Deutsche Mark.\"\n",
    "d[6][\"description\"] = \"Present employment, in number of years.\"\n",
    "d[7][\"description\"] = \"Installment rate in percentage of disposable income\"\n",
    "d[8][\"description\"] = \"Personal status (married, single,...) and sex\"\n",
    "d[9][\"description\"] = \"Other debtors / guarantors\"\n",
    "d[10][\"description\"] = \"Present residence since X years\"\n",
    "d[11][\"description\"] = \"Property (e.g. real estate)\"\n",
    "d[12][\"description\"] = \"Age in years\"\n",
    "d[13][\"description\"] = \"Other installment plans (banks, stores)\"\n",
    "d[14][\"description\"] = \"Housing (rent, own,...)\"\n",
    "d[15][\"description\"] = \"Number of existing credits at this bank\"\n",
    "d[16][\"description\"] = \"Job\"\n",
    "d[17][\"description\"] = \"Number of people being liable to provide maintenance for\"\n",
    "d[18][\"description\"] = \"Telephone (yes,no)\"\n",
    "d[19][\"description\"] = \"Foreign worker (yes,no)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Finally we store the all data in a data holder class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% \t\n"
    }
   },
   "outputs": [],
   "source": [
    "dataset = Dataset(D, X, y, C, F, y_labels, X_train, y_train, X_test, y_test, dfeature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## The model\n",
    "The next step is to fit a machine-learning model on the data. In this example we use scikit-learn's\n",
    "RandomForest classifier. Since we use black-box explainer LIME it does not matter which model we use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Predictive accuracy: 0.73\nClassification report for classifier Pipeline(steps=[('preprocessor',\n                 ColumnTransformer(transformers=[('cat',\n                                                  Pipeline(steps=[('onehot',\n                                                                   OneHotEncoder(handle_unknown='ignore'))]),\n                                                  [True, False, True, True,\n                                                   False, True, True, False,\n                                                   True, True, False, True,\n                                                   False, True, True, False,\n                                                   True, False, True,\n                                                   True])])),\n                ('classifier',\n                 RandomForestClassifier(max_depth=10, max_features=15,\n                                        max_leaf_nodes=16, n_estimators=250,\n                                        n_jobs=-1))]):\n              precision    recall  f1-score   support\n\n           0       0.79      0.87      0.83       108\n           1       0.42      0.29      0.34        35\n\n    accuracy                           0.73       143\n   macro avg       0.60      0.58      0.58       143\nweighted avg       0.70      0.73      0.71       143\n\n\nConfusion matrix:\n[[94 14]\n [25 10]]\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# define pipeline\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "\t('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "preprocessor = ColumnTransformer(\n",
    "\ttransformers=[\n",
    "\t\t('cat', categorical_transformer, dataset.C)\n",
    "\t]\n",
    ")\n",
    "rf = RandomForestClassifier(n_estimators=250, max_features=15, max_depth=10, max_leaf_nodes=16, n_jobs=-1)\n",
    "pipe = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "\t\t\t\t\t ('classifier', rf)])\n",
    "\n",
    "# fit pipeline\n",
    "pipe.fit(dataset.X_train, dataset.y_train)\n",
    "\n",
    "# store results\n",
    "y_pred = pipe.predict(dataset.X_test)\n",
    "dataset.setModel(pipe, y_pred)\n",
    "\n",
    "# print metrics\n",
    "dataset.printMetrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Explanation case\n",
    "\n",
    "Let's pick a case that we want to explain. We pick a random case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% \n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "case id: 137\nlabel: good\nprediction: good\nfeatures:\n                        values\nchecking_status            3.0\nduration                  12.0\ncredit_history             2.0\npurpose                    3.0\ncredit_amount           3059.0\nsavings_status             3.0\nemployment                 3.0\ninstallment_commitment     2.0\npersonal_status            0.0\nother_parties              0.0\nresidence_since            4.0\nproperty_magnitude         0.0\nage                       61.0\nother_payment_plans        2.0\nhousing                    1.0\nexisting_credits           1.0\njob                        1.0\nnum_dependents             1.0\nown_telephone              0.0\nforeign_worker             0.0\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "def pickCase() -> int:\n",
    "\treturn np.random.randint(0, dataset.y_test.shape[0])\n",
    "\n",
    "def printCase(case_id: int) -> None:\n",
    "\tprint(\"case id: \" + str(case_id))\n",
    "\tprint(\"label:\", dataset.y_labels[dataset.y_test[case_id]])\n",
    "\tprint(\"prediction:\", dataset.y_labels[dataset.y_pred[case_id]])\n",
    "\tprint(\"features:\")\n",
    "\tdf = pd.DataFrame(data=dataset.X_test[case_id], index=dataset.F, columns=['values'])\n",
    "\tprint(df)\n",
    "\t\n",
    "case_id = 21 #pickCase()\n",
    "case = Case({\n",
    "\t\"id\": case_id,\n",
    "\t\"class_proba\": dataset.m.predict_proba(dataset.X_test)[case_id].tolist(),\n",
    "\t\"sources\": [CaseSource({\n",
    "\t\t\"features\": list(map(lambda x: CaseFeature({\"value\": x}), dataset.X_test[case_id].tolist()))\n",
    "\t})]\n",
    "})\n",
    "printCase(case_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Run Anchor Explainer\n",
    "\n",
    "Now that we have picked a case we can run [Anchor](https://github.com/marcotcr/anchor) to generate\n",
    "feature-importance weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "# convert category values to Anchor format\n",
    "categorical_names = {}\n",
    "for i in range(0, len(d)):\n",
    "\tif d[i][\"data_type\"] == \"nominal\":\n",
    "\t\tcategorical_names[i] = d[i][\"nominal_value\"]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-4050a6485004>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# generate explanation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m exp = explainer.explain_instance(dataset.X_test[case_id],\n\u001b[0m\u001b[1;32m     10\u001b[0m                                                                  \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                                                                  \u001b[0mencoder_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpreprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/ArgueView-PiOJAf3v/lib/python3.8/site-packages/anchor/anchor_tabular.py\u001b[0m in \u001b[0;36mexplain_instance\u001b[0;34m(self, data_row, classifier_fn, threshold, delta, tau, batch_size, max_anchor_size, desired_label, beam_size, **kwargs)\u001b[0m\n\u001b[1;32m    276\u001b[0m             data_row, classifier_fn, desired_label=desired_label)\n\u001b[1;32m    277\u001b[0m         \u001b[0;31m# return sample_fn, mapping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 278\u001b[0;31m         exp = anchor_base.AnchorBaseBeam.anchor_beam(\n\u001b[0m\u001b[1;32m    279\u001b[0m             \u001b[0msample_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdelta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtau\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m             \u001b[0mdesired_confidence\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_anchor_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_anchor_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: anchor_beam() got an unexpected keyword argument 'encoder_fn'"
     ],
     "ename": "TypeError",
     "evalue": "anchor_beam() got an unexpected keyword argument 'encoder_fn'",
     "output_type": "error"
    }
   ],
   "source": [
    "# create explainer\n",
    "explainer = anchor_tabular.AnchorTabularExplainer(\n",
    "    dataset.y_labels,\n",
    "    dataset.F,\n",
    "    dataset.X_train,\n",
    "    categorical_names)\n",
    "\n",
    "# generate explanation\n",
    "exp = explainer.explain_instance(dataset.X_test[case_id],\n",
    "\t\t\t\t\t\t\t\t dataset.m.predict,\n",
    "\t\t\t\t\t\t\t\t threshold=0.95)\n",
    "\n",
    "# preview\n",
    "print('Anchor: %s' % (' AND '.join(exp.names())))\n",
    "print('Precision: %.2f' % exp.precision())\n",
    "print('Coverage: %.2f' % exp.coverage())\n",
    "\n",
    "# save\n",
    "exp.show_in_notebook()\n",
    "\n",
    "# convert anchor's output to feature import map interpretable by argueview\n",
    "feature_importance, unexplained = feature_importance_from_anchor(exp, case)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Build ArgueView model\n",
    "After we successfully obtained feature importance we can start to build the argueview explanation\n",
    "model. ArgueView is primarily a text-based explanation presentation tool. It's focus lies on\n",
    "explaining decisions to end-users or laymen. Therefore, ArgueView relies heavily on textual input.\n",
    "\n",
    "ArgueView contains a set of input methods to help you define text-based explanations for each case\n",
    "scenario. In the current section we will illustrate how to build a text-based explanation model\n",
    "for the CreditG dataset we use throughout this example.\n",
    "\n",
    "### Toulmin's model of argumentation\n",
    "ArgueView's explanation model is inspired by [Toulmin's model of argumentation](https://owl.purdue.edu/owl/general_writing/academic_writing/historical_perspectives_on_argumentation/toulmin_argument.html) \n",
    "(see van Eemeren, 2014). The following figure illustrates a possible model.\n",
    "\n",
    "![CreditG Toulmin model](https://github.com/sophiahadash/argueview/blob/master/screenshots/toulmin-creditg.png?raw=true)\n",
    "\n",
    "<sup>van Eemeren, F. H., Garssen, B., Krabbe, E. C. W., Snoeck Henkemans, A. F., Verheij, B., & Wagemans, J. H. M. (2014). Handbook of Argumentation Theory. https://doi.org/10.1007/978-90-481-9473-5</sup>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "argView = ArgueView()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Define classes\n",
    "First, we have to define textual representations of our classes. Try to use names that are not \n",
    "too abstract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "argView.classes([\"You are applicable for a loan.\", \"You are not applicable for a loan.\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define latent continuous target\n",
    "In the case of a decision task where there is a latent continuous target we can define it \n",
    "such that ArgueView can improve it's explanations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "argView.latent_continuous_target(\"applicability\", \"inapplicability\", [1, -1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Define backing\n",
    "Next, we define a backing. A backing should include why someone should trust your explanation. It can\n",
    "contain descriptions of model accuracy, organizational credibility, or data credibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "argView.backing(\"Supported by Sophia Hadash, MSc from Jheronimus Academy of Data Science.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Define data source\n",
    "Then we must add information on our data sources. In this example we only used one data source. This\n",
    "information is obtained from the [OpenML data wiki](https://www.openml.org/d/31)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "argView.add_data_source(Source({\n",
    "\t\"name\": 'German Credit',\n",
    "\t\"author\": 'Dr. Hans Hofmann, Universität Hamburg, Institut für Statistik und Ökonometrie',\n",
    "\t\"description\": 'This dataset classifies people described by a set of attributes as good or bad credit risks.',\n",
    "\t\"href\": 'https://archive.ics.uci.edu/ml/datasets/statlog+(german+credit+data)',\n",
    "\t\"observations\": 1000,\n",
    "\t\"type\": 'static',\n",
    "\t\"year\": 1994,\n",
    "\t\"features\": dataset.feature_data.feature\n",
    "}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Define grounds\n",
    "The most important step is to define your grounds. The grounds specify how each feature potentially \n",
    "contributes to the varying decision classes. For each feature we must describe why/how it \n",
    "hypothetically contributes to the decision classes. We describe this in a 2D-list, where the outer list\n",
    "contains entries for each feature and the inner list contains an explanation for each decision-class.\n",
    "\n",
    "Furthermore, these explanations are tokenized by feature value. This allows the explanations to vary\n",
    "depending on both decision-class and feature value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "argView.grounds([\n",
    "\t[\"You have a sufficient amount on your checking account.\",\n",
    "\t \"You have an insufficient amount on your checking account.\"],\n",
    "\t[\"We believe the duration of the credit is appropriate.\",\n",
    "\t \"We believe the duration of the credit is not appropriate.\"],\n",
    "\t[\"Your credit history gives us confidence in your capabilities.\",\n",
    "\t \"Your credit history does not give us confidence in your capabilities.\"],\n",
    "\t[\"We are interested in providing loans for <>.\",\n",
    "\t \"We are generally not interested in providing loans for <>.\"],\n",
    "\t[\"Your credit amount gives us confidence in your capabilities.\",\n",
    "\t \"Your credit amount does not give us sufficient confidence in your capabilities.\"],\n",
    "\t[\"Your savings give us confidence in your capabilities.\",\n",
    "\t \"Your savings do not give us confidence in your capabilities.\"],\n",
    "\t[\"The duration of your current employment gives us confidence in your capabilities.\",\n",
    "\t \"The duration of your current employment does not give us confidence in your capabilities.\"],\n",
    "\t[\"With the requested loan included, your installment rate is below the threshold.\",\n",
    "\t \"The requested loan would increase your EMI over the threshold.\"],\n",
    "\t[\"We have more confidence in providing loans to <> in general.\",\n",
    "\t \"We have less confidence in providing loans to <> in general.\"],\n",
    "\t[\"<> gives us confidence in your capabilities.\",\n",
    "\t \"<> does not give us sufficient confidence in your capabilities.\"],\n",
    "\t[\"The duration of your current residence gives us confidence in your capabilities.\",\n",
    "\t \"The duration of your current residence does not give us confidence in your capabilities.\"],\n",
    "\t[\"<> gives us confidence in your capabilities.\", \"<> does not give us confidence in your capabilities.\"],\n",
    "\t[\"Your age gives us confidence in your capabilities.\",\n",
    "\t \" Your age does not give us confidence in your capabilities.\"],\n",
    "\t[\"<> gives us confidence in your capabilities.\", \"<> does not give us confidence in your capabilities.\"],\n",
    "\t[\"Living in <> increases the confidence we have in your capabilities.\",\n",
    "\t \"Living in <> decreases the confidence we have in your capabilities.\"],\n",
    "\t[\"The amount of your credits gives us confidence in your capabilities.\",\n",
    "\t \"The amount of your credits does not give us confidence in your capabilities.\"],\n",
    "\t[\"Your current employment responsibilities supports our confidence in your capabilities.\",\n",
    "\t \"Your current employment responsibilities give us less confidence in your capabilities.\"],\n",
    "\t[\"The number of people that are liable to provide maintenance for gives us confidence in your capabilities.\",\n",
    "\t\"The number of people that are liable to provide maintenance for does not give us confidence in your capabilities.\"],\n",
    "\t[\"The <> telephone gives us confidence in your capabilities.\",\n",
    "\t \"The <> telephone does not give us confidence in your capabilities.\"],\n",
    "\t[\"Because you are <>a foreign worker, we have more confidence in your capabilities.\",\n",
    "\t \"Because you are <>a foreign worker, we have less confidence in your capabilities.\"],\n",
    "], [\n",
    "\t[],\n",
    "\t[],\n",
    "\t[],\n",
    "\t[\"the purpose of buying a new car\", \"the purpose of buying a used car\",\n",
    "\t \"the purpose of buying furniture or equipment\", \"the purpose of buying a radio or tv\",\n",
    "\t \"the purpose of buying domestic appliance\", \"the purpose of making repairs\",\n",
    "\t \"the purpose of gaining education\", \"the purpose of paying for vacation\", \"the purpose of retraining\",\n",
    "\t \"the purpose of investing in business\", \"unspecified purposes\"],\n",
    "\t[],\n",
    "\t[],\n",
    "\t[],\n",
    "\t[],\n",
    "\t[\"divorced or separated males\", \"females\", \"single males\", \"married or widowed males\", \"single females\"],\n",
    "\t[\"The lack of a co-applicant or guarantor\", \"Your co-applicant\", \"Your guarantor\"],\n",
    "\t[],\n",
    "\t[\"Your real-estate\", \"Your life insurance\", \"The fact that you own a car\", \"The absence of known property\"],\n",
    "\t[],\n",
    "\t[\"Your installment plan at a bank\", \"Your installment plan at a store\",\n",
    "\t \"Having no other installment plans\"],\n",
    "\t[\"rented housing\", \"owned housing\", \"free housing\"],\n",
    "\t[],\n",
    "\t[],\n",
    "\t[],\n",
    "\t[\"lack of ownership of a\", \"ownership of a\"],\n",
    "\t[\"\", \"not \"],\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Generate explanation\n",
    "After the argueview explanation model is built, we are ready to generate our explanation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "explanation = argView.generate(case, feature_importance, unexplained)\n",
    "explanation.print(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We can also show the explanation using the built-in visualization tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ToulminVisualizer(explanation)\n",
    "FeatureListVisualizer(explanation, visualization=\"bar\", framing=\"positive\", lct=\"label\", threshold_badge=0, threshold_omit=0.005)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
